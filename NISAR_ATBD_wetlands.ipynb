{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41cfff41",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/60/NISAR_artist_concept.jpg\" width=400 align=\"left\"/><br><br><br><br>\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/9/9b/NISAR_Mission_Logo.png\" width=400 align=\"left\"/><br><br><br><br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0df5663",
   "metadata": {},
   "source": [
    "# Notebook: NISAR ATBD for Classification of Wetland Inundation Extent\n",
    "## NASA ISRO Synthetic Aperture Radar Mission\n",
    "### Combined Algorithm Theoretical Basis Document and Jupyter Notebook for <br> *Classification of Wetland Inundation Extent*\n",
    "\n",
    "Authors: Bruce Chapman, Paul Siqueira\n",
    "\n",
    "Date: February 15, 2022 Cloud Best Practices and Gotchas\n",
    "\n",
    "Modified by Alex Lewandowski, 2024-10-04\n",
    "\n",
    "*Changes made for the NISAR Early Adopters Workshop, October 2024*\n",
    "\n",
    "### Summary\n",
    "This notebook describes the ATBD for generating a wetland inundation product from NISAR time series data stacks. First, the images of the multi-temporal sequence must be well radiometrically calibrated relative to each other, to a higher precision than perhaps required through routine standard calibration of the NISAR imagery. This optional calibration step examines distributed targets that are expected to be unchanged or minimally changed in brightness over a set time span of  an image sequence. With NISARâ€™s 240 km swath width, it is reasonably assumed that a statistically large area, A<sub>ni</sub>, will not be inundated (or otherwise changing) during any of the 2n observations surrounding the image to be calibrated and classified. These areas will be identified through use of a priori wetlands mask and partly through image segmentation or other methods over the 2n images. <br>\n",
    "A set of classes will be identified from a multitemporal average of a subset of images including:\n",
    "\n",
    "- Inundated vegetation (presumption: dominated by double bounce scatter in HH channel) \n",
    "- Open water (presumption: low specular scattering in both channels)\n",
    "- Not inundated (presumption: brighter specular scatter, volume scattering)\n",
    "- Not classified (presumption: pixels do not align with the scattering model, or no data)\n",
    "\n",
    "These classes are selected based on calibrated threshold values for the radar backscatter and other metrics. In addition, this same multi-temporal image sequence allows the algorithm to include a more sensitive change detection component for improved robustness. Change detection will allow for refinement within the multitemporal image sequence for change of class during the image sequence that may be more robust than simply classifying the image backscatter and backscatter ratio values.  \n",
    "\n",
    "Still to be implemented in notebook form:  \n",
    "\n",
    "- Incorporating datatakes from both ascending and descending orbits with varying incidence angles<br>\n",
    "- Incorporating datatakes that only have HH channel (no HV)<br>\n",
    "- Aggregating results to 1 ha<br>\n",
    "- Evalation against reference data set<br>\n",
    "- Calibration of thresholds with reference data set<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae80bb4b-b393-401f-8cfe-dc5a3ddcfe09",
   "metadata": {},
   "source": [
    "## `git` is the best!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6e491b-87ae-4b2b-89df-e3906db91d7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"TOC\"></a>\n",
    "### Table of Contents\n",
    "0. [Getting Started](#SEC_0)\n",
    "    <br>0.1 [Import Packages](#SEC_0.1)\n",
    "    <br>0.2 [Set the Input Parameters](#SEC_0.2)\n",
    "1. [Get Data](#SEC_1)\n",
    "    <br>1.1  [Pre-Launch: Get GCOV data from S3 Buckets](#SEC_1.1)\n",
    "    <br>1.2  [Post-Launch: Query EarthData](#SEC_1.2)\n",
    "    <br>1.3  [Post-Launch: Query ASF](#SEC_1.3)\n",
    "    <br>1.4  [Select Images to Include in Time-Series Stack](#SEC_1.4)\n",
    "    <br>1.5  [Get Geocoding Information from First/Reference Images](#SEC_1.5)\n",
    "2.  [Process Imagery](#SEC_2)\n",
    "    <br>2.1  [Read in imagery, with option to combine with band B](#SEC_2.1)\n",
    "    <br>2.2  [Option: spatial rolling average ](#SEC_2.2)\n",
    "    <br>2.3  [Option: temporal rolling average of a short time sequence of imagery](#SEC_2.3)\n",
    "    4.  [Option: calculate correction factors to insure images are relatively calibrated to one another overall](#SEC2.4)\n",
    "3.  [Classification](#SEC_3)\n",
    "    <br>3.1  [Set thresholds and classify mean image](#SEC_3.1)\n",
    "    <br>3.2  [Classify Time Series](#SEC_3.2)\n",
    "    <br>3.3  [Examine change relative to means for each time stamp to refine classes](#SEC_3.3)\n",
    "4. [Output Results](#SEC_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43044dea",
   "metadata": {},
   "source": [
    "<a id=\"SEC_0\"></a>\n",
    "<a id=\"SEC_0.1\"></a>\n",
    "# 0  &emsp; Getting Started\n",
    "## 0.1 &emsp; Import Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c478e4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import re\n",
    "import datetime\n",
    "import shutil\n",
    "import json\n",
    "from getpass import getpass\n",
    "\n",
    "import rasterio\n",
    "from rasterio.plot import show_hist\n",
    "import struct\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import geopandas as gpd\n",
    "import h5py\n",
    "from pyproj import CRS\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import colors\n",
    "\n",
    "import asf_search as asf\n",
    "import boto3\n",
    "import earthaccess\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84caa18c-0194-40aa-b92f-ce5cda1c9fe7",
   "metadata": {},
   "source": [
    "<a id=\"SEC_1\"></a>\n",
    "# 1 &emsp; Get Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c583987-b9f1-47a2-964f-7a8902860f3e",
   "metadata": {},
   "source": [
    "<a id=\"SEC_1.1\"></a>\n",
    "# 1.1  &emsp; Pre-Launch: Get GCOV Data from S3 Buckets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7b925e-616b-42d1-970c-326def436808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "aoi = 'Missippi_Vicksburg_MS'\n",
    "aoi_dir = Path.cwd() / aoi \n",
    "atbd_dir = aoi_dir / 'wetlands' # directory doesn't seem to be used\n",
    "GCOV_dir = aoi_dir / 'GCOV'\n",
    "TMP_dir = aoi_dir / 'TMP' # directory doesn't seem to be used\n",
    "\n",
    "for path in [aoi_dir, atbd_dir, GCOV_dir, TMP_dir]:\n",
    "    path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550ff5e2-9f51-4813-abf2-cdfad35a8135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "bucket_name = \"nisar-calval-public-west2\"\n",
    "bucket_name = \"asf-jupyter-data-west\"\n",
    "\n",
    "prefix = 'wetlands/NISAR'\n",
    "\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "for obj in bucket.objects.filter(Prefix=prefix, RequestPayer='requester'):\n",
    "    print(f's3://{bucket_name}/{obj.key}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee94d7a-8f92-4035-aee7-b90db28077b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in bucket.objects.filter(Prefix=prefix, RequestPayer='requester'):\n",
    "    bucket.download_file(obj.key, GCOV_dir/obj.key.split('/')[-1], ExtraArgs={'RequestPayer': 'requester'})\n",
    "list(GCOV_dir.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702b3515-7af8-42ed-8b94-95689ea5d169",
   "metadata": {},
   "source": [
    "<a id=\"SEC_1.2\"></a>\n",
    "# 1.2  &emsp; Post-Launch: Query EarthData with **earthaccess** for GCOV Data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf48533-b746-4cc9-a33f-c6e9c36c28d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# auth = earthaccess.login()\n",
    "\n",
    "# box_left, box_top, box_right, box_bottom = extent.total_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64395f79-b28b-41d6-8bb0-2034c8d73f63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nisar_results = earthaccess.search_data(short_name = 'NISAR_L2_GCOV_BETA_V1', \n",
    "#                                         # temporal = ('2023-07-01 00:00:00', '2023-08-31 23:59:59'), # can also specify by time\n",
    "#                                         # granule_name = '*T00888*', # here we filter by files with CRID value of *T00888*\n",
    "#                                         bounding_box = (box_left, box_top, box_right, box_bottom ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200389ef-b098-4b99-b1ee-0a64d5929b86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download_files = earthaccess.download(nisar_results, GCOV_dir) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58836a48-7697-46cc-89ea-021393ef997b",
   "metadata": {},
   "source": [
    "<a id=\"SEC_1.3\"></a>\n",
    "# 1.3  &emsp; Post-Launch: Query ASF with **asf_search** for GCOV Data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23857b6-0fd7-4125-9828-fca47da749e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session = asf.ASFSession()\n",
    "# Session.auth_with_token(getpass(\"EDL Token:\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abff5fb-d0be-46ba-a9be-4d3f6e8bbc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opts = asf.ASFSearchOptions(\n",
    "#     dataset=asf.DATASET.NISAR,\n",
    "#     session=Session, \n",
    "#     )\n",
    "# gdf = json.loads(open('%s/extent.geojson' %(aoi_dir)).read())\n",
    "# wkt = str('Polygon((')\n",
    "# points = len(gdf['features'][0]['geometry']['coordinates'][0][0])\n",
    "# for i in range(points):\n",
    "#     wkt = wkt + str(gdf['features'][0]['geometry']['coordinates'][0][0][i][0]) + ' ' + str(gdf['features'][0]['geometry']['coordinates'][0][0][i][1]) \n",
    "#     if i<points-1:\n",
    "#         wkt = wkt + ','\n",
    "# wkt = wkt + '))'\n",
    "# results = asf.geo_search(intersectsWith=wkt, opts=opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573dba75-ddbd-44b7-b191-4117ceebe4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# list_of_NISAR_GCOVs = [results.geojson()['features'][x]['properties']['url'] for x in range(len(results.geojson()['features']))]\n",
    "# # print('\\n'.join(list_of_NISAR_GCOVs))\n",
    "\n",
    "# frames = list([f.split('/')[-1].split('_')[7] for f in list_of_NISAR_GCOVs])\n",
    "# uni_frames = list(np.unique([f.split('/')[-1].split('_')[7] for f in list_of_NISAR_GCOVs]))\n",
    "# print('\\n'.join(uni_frames))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b542b1e-8a39-4df1-8027-3a218538812a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import fsspec\n",
    "\n",
    "# s3_path = 's3://sds-n-cumulus-prod-nisar-products/NISAR_L2_GCOV_BETA_V1/NISAR_L2_PR_GCOV_001_001_A_150_2000_SHNA_A_20231023T060152_20231023T060202_T00787_M_F_J_787/NISAR_L2_PR_GCOV_001_001_A_150_2000_SHNA_A_20231023T060152_20231023T060202_T00787_M_F_J_787.h5'\n",
    "\n",
    "# s3f = fsspec.open(s3_path, mode='rb', anon=True, default_fill_cache=False)\n",
    "# h5f = h5py.File(s3f.open(), mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af118902-b984-4547-b9ea-c32e9192e0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame = input('which frame do you want to process? %s' %(uni_frames))\n",
    "# # list_of_NISAR_GCOVs = [k for k in all_results if frame in k]\n",
    "# indices = [frames.index(k) for k in frames if frame in k]\n",
    "# # [list_of_NISAR_GCOVs[i] for i in indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69ba11d-afb0-4d03-b4af-f093daaa06a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NISAR_ids = []\n",
    "# response = s3.list_objects_v2(\n",
    "#             Bucket=bucket_name,\n",
    "#             Prefix = s3_path)\n",
    "#             # MaxKeys=100)\n",
    "#             # Delimiter = '/')\n",
    "# contents = response.get('Contents')\n",
    "# existing_NISAR_hdf5 = [contents[i].get('Key').split('/')[-1] for i in range(len(contents)) if '.h5' in contents[i].get('Key')]\n",
    "# existing_NISAR_hdf5paths = [contents[i].get('Key') for i in range(len(contents)) if '.h5' in contents[i].get('Key')]\n",
    "\n",
    "# for i in indices:\n",
    "#     filename =  list_of_NISAR_GCOVs[i].split('/')[-1]\n",
    "#     NISAR_ids.append(filename)\n",
    "#     print('Requested File: ', filename)\n",
    "#     if os.path.isdir(GCOV_dir/filename.split('/')[-1][:-4])==True & (filename in existing_NISAR_hdf5)==True:\n",
    "#         print('NISAR file is stored on S3 and already available locally')\n",
    "#     elif (os.path.isfile(GCOV_dir/filename)==False) & (filename in existing_NISAR_hdf5):\n",
    "#         i = existing_NISAR_hdf5.index(filename)\n",
    "#         s3_path_new = existing_NISAR_hdf5paths[i]\n",
    "#         print('\\tNISAR HDF5 is already available at S3 PATH: ', s3_path_new)\n",
    "#         print('\\tMove NISAR HDF5 from S3 to local')\n",
    "#         s3.download_file(bucket_name, s3_path_new , GCOV_dir/filename)\n",
    "        \n",
    "#     elif (os.path.isfile(GCOV_dir/filename)==False) & (filename not in existing_NISAR_hdf5):\n",
    "#         print('\\tNISAR HDF5 is not available anywhere')\n",
    "#         print('\\tDownloading NISAR HDF5')\n",
    "#         try:\n",
    "#             results[i].download(path=GCOV_dir)#, session=session)\n",
    "#         except:\n",
    "#             print('Your .netrc file is not configured. We will authenticate a session with your username and password')\n",
    "#             user = input('What is your earthdata username?')\n",
    "#             psw = input('What is your earthdata password?')\n",
    "#             # session = asf.ASFSession().auth_with_creds(user, pwd)\n",
    "#             results[i].download(path=GCOV_dir, session=Session)\n",
    "#         print('\\tMoving a copy NISAR HDF5 to S3 bucket')\n",
    "#         s3.upload_file(Filename= str(GCOV_dir / filename), Bucket=bucket_name, Key='%s%s/%s' %(s3_path,aoi,filename))\n",
    "\n",
    "#     elif (os.path.isfile(GCOV_dir/filename)==True) & (filename not in existing_NISAR_hdf5):\n",
    "#         print('\\tNISAR HDF5 is available locally, but not on S3')\n",
    "#         print('\\tMoving a copy NISAR HDF5 to S3 bucket')\n",
    "#         s3.upload_file(Filename= str(GCOV_dir / filename), Bucket=bucket_name, Key='%s%s/%s' %(s3_path,aoi,filename))\n",
    "\n",
    "#     else: \n",
    "#         print('\\tNISAR HDF5 file exists locally and on S3')\n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5197571-c891-4cb0-b813-464bdcf51584",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"SEC_1.4\"></a>\n",
    "# 1.4  &emsp; Select Images to Include in Time-Series Stack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9e1ff8-b475-4cc3-a06c-6243b5397338",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_SAR_data = sorted(glob.glob(str(GCOV_dir / '*.h5')))\n",
    "\n",
    "print('\\n'.join([i.split('/')[-1] for i in all_SAR_data]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431949b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices = range(0,len(all_SAR_data))\n",
    "## If you don't want to use all of the images, choose which indices to use now. \n",
    "indices = range(0,3)\n",
    "\n",
    "\n",
    "num=indices[0]\n",
    "print(\"Dates of observation (HH and HV):\")\n",
    "date_array = []\n",
    "SAR_images = []\n",
    "for ii in indices:\n",
    "    datestr = all_SAR_data[ii].split('/')[-1].split('_')[11][:8]\n",
    "   \n",
    "    date_obj = datetime.datetime.strptime(datestr,'%Y%m%d')\n",
    "    print(ii, '\\t',date_obj)\n",
    "    date_array.append(date_obj)\n",
    "    SAR_images.append(all_SAR_data[ii])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8b067e-90f6-4e6b-8876-513dc6ee59c3",
   "metadata": {},
   "source": [
    "<a id=\"SEC_1.3\"></a>\n",
    "# 1.5  &emsp; Get Geocoding Information from First/Reference Images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3466c4a7-1b99-4114-8a21-1cef77e526ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ref_image = all_SAR_data[0]\n",
    "f = h5py.File(ref_image, \"r\") \n",
    "a_group_key = list(f.keys())[0]\n",
    "ds_x = f[a_group_key]['LSAR']['GCOV']['grids']['frequencyA']['xCoordinates'][()]      # returns as a h5py dataset object\n",
    "ds_y = f[a_group_key]['LSAR']['GCOV']['grids']['frequencyA']['yCoordinates'][()]      # returns as a h5py dataset object\n",
    "ds_epsg = f[a_group_key]['LSAR']['GCOV']['grids']['frequencyA']['projection'][()]\n",
    "cols = ds_x.shape[0]\n",
    "rows = ds_y.shape[0]\n",
    "print('X Size: ',cols,' Y Size: ',rows)\n",
    "\n",
    "yres = abs(ds_y[0] - ds_y[1])\n",
    "xres = abs(ds_x[0] - ds_x[1])\n",
    "print('Resolution X:', xres, ' Y:',yres,'m')\n",
    "\n",
    "# ref_te = ds_x[0],ds_x[-1],ds_y[0],ds_y[-1]\n",
    "ulx = xres * round((ds_x[0] - ((ds_x[1] - ds_x[0])/2))/xres)\n",
    "lrx = xres * round(ds_x[-1]/xres)\n",
    "uly = yres * round((ds_y[0] - ((ds_y[1] - ds_y[0])/2))/yres)\n",
    "lry = yres * round(ds_y[-1]/yres)\n",
    "print('Raster bounds: ',ulx,lrx,uly,lry)#min(ds_x),max(ds_x),min(ds_y),max(ds_y))\n",
    "crop = False\n",
    "if crop:\n",
    "    # crop = gpd.read_file(aoi_dir / 'crop.geojson')\n",
    "    ulx,lry,lrx,uly = 664836.2728999999817461,3528994.8253000001423061,690193.6014999999897555,3560044.6154000000096858\n",
    "    print('Crop bounds: ', ulx,lrx,uly,lrx)\n",
    "geotransform = [ulx, xres, 0.0, uly, 0.0, -yres]\n",
    "\n",
    "spatialref = CRS.from_user_input(ds_epsg).to_wkt()\n",
    "print(ds_epsg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ab235d-298b-4769-9f81-4bbdc9514804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_lon = np.meshgrid(ds_x,ds_y)[0]\n",
    "tmp_lat = np.meshgrid(ds_x,ds_y)[1]\n",
    "crop_xy = np.where((tmp_lon>ulx) & (tmp_lon <lrx) & (tmp_lat >lry) & (tmp_lat <uly),1,0)\n",
    "coords = np.argwhere(crop_xy)\n",
    "x_min, y_min = coords.min(axis=0)\n",
    "x_max, y_max = coords.max(axis=0)\n",
    "cropped = crop_xy[x_min:x_max+1, y_min:y_max+1]\n",
    "\n",
    "cols = cropped.shape[1]\n",
    "rows = cropped.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140fa175",
   "metadata": {},
   "source": [
    "## Water Level from nearby USGS water level gage with above image acquisition dates, showing the change in water level during the course of those acquisitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfecb21c",
   "metadata": {},
   "source": [
    "https://rivergages.mvr.usace.army.mil/WaterControl/stationinfo2.cfm?sid=CE40FF58&fid=VCKM6&dt=S\n",
    "\n",
    "<img style=\"float: left;\" src=\"images/water_level_chart.png\"><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63586f1c",
   "metadata": {},
   "source": [
    "<a id=\"SEC_2\"></a>\n",
    "<a id=\"SEC_2.1\"></a>\n",
    "# 2 &emsp; Process Imagery\n",
    "# 2.1  &emsp; Read in imagery, with option to combine with band B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caba03e3-b742-48f3-8b8f-5e127dcc3131",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mask = np.ones((rows,cols)) \n",
    "\n",
    "# Read raster files and make them into a 3D numpy array\n",
    "HH = []\n",
    "HV = []\n",
    "\n",
    "for image in SAR_images:\n",
    "    print(image.split('/')[-1])\n",
    "    f = h5py.File(image, \"r\") \n",
    "    a_group_key = list(f.keys())[0]\n",
    "    ds_HH = f[a_group_key]['LSAR']['GCOV']['grids']['frequencyA']['HHHH'][()] \n",
    "    ds_HV = f[a_group_key]['LSAR']['GCOV']['grids']['frequencyA']['HVHV'][()] \n",
    "    ds_HH = ds_HH[x_min:x_max+1, y_min:y_max+1]\n",
    "    ds_HV = ds_HV[x_min:x_max+1, y_min:y_max+1]\n",
    "    if (ds_HH.shape[1] == cols) & (ds_HH.shape[0] == rows):\n",
    "        ftemp = np.clip(ds_HH,0.0,10000.0)\n",
    "        HH.append(ftemp)\n",
    "        ftemp = np.clip(ds_HV,0.0,10000.0)\n",
    "        HV.append(ftemp)\n",
    "    else:\n",
    "        print('Dimensions of this SAR image do not match the reference image, SKIP')\n",
    "rasterstack_HH = np.array(HH, dtype=float)\n",
    "rasterstack_HV = np.array(HV, dtype=float)\n",
    "\n",
    "#option: combine band A and band B by multilooking\n",
    "#for some reason, band B is in the opposite order\n",
    "# for ii in range(len(rasterstack_HH)):\n",
    "#     rasterstack_HH[ii,:,:]=(rasterstack_HH[ii,:,:]+rasterstack_HH_B[num-ii,:,:])/2.0\n",
    "#     rasterstack_HV[ii,:,:]=(rasterstack_HV[ii,:,:]+rasterstack_HV_B[num-ii,:,:])/2.0\n",
    "    \n",
    "#option: Classify band B\n",
    "#for some reason, band B is in the opposite order\n",
    "#for ii in range(len(rasterstack_HH)):\n",
    "#    rasterstack_HH[ii,:,:]=rasterstack_HH_B[num-ii,:,:]\n",
    "#    rasterstack_HV[ii,:,:]=rasterstack_HV_B[num-ii,:,:]\n",
    "    \n",
    "ss=np.zeros(3,dtype=np.int16)\n",
    "ss[0]=int(rasterstack_HV.shape[0])\n",
    "ss[1]=int(rasterstack_HV.shape[1])\n",
    "ss[2]=int(rasterstack_HV.shape[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f6357d",
   "metadata": {},
   "source": [
    "<a id=\"SEC_2.2\"></a>\n",
    "# 2.2  &emsp; Option: spatial rolling average "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc83dfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#5x5\n",
    "#ispat=0 - no spatial averaging\n",
    "#ispat=1 - execute spatial averaging comparable to output pixel spacing (ie about 5x5)\n",
    "#  -- but UAVSAR pixel spacing is just 6 m, so this correponds to an output pixel spacing of 30 m.\n",
    "ispat=1\n",
    "if ispat == 1:\n",
    "    ks = 5\n",
    "    kernel=(ks,ks)\n",
    "    kernel = np.ones(kernel) /(ks*ks)\n",
    "    for kk in range(ss[0]):\n",
    "        rasterstack_HH[kk,:,:] = signal.convolve2d(rasterstack_HH[kk,:,:], kernel, mode='same')\n",
    "        rasterstack_HV[kk,:,:] = signal.convolve2d(rasterstack_HV[kk,:,:], kernel, mode='same')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c65c0a",
   "metadata": {},
   "source": [
    "<a id=\"SEC_2.3\"></a>\n",
    "# 2.3  &emsp; Option: temporal rolling average of a short time sequence of imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18d8019",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#if temporal rolling average is not desired, set Nave_rolling =1 below\n",
    "#number of images in temporal rolling average\n",
    "Nave_rolling = 1\n",
    "\n",
    "np.seterr(divide='ignore')  # python doesn't like dividing zero by zero, NaN by NaN, etc.\n",
    "np.seterr(invalid='ignore')\n",
    "\n",
    "ss = rasterstack_HH.shape\n",
    "#number of rolling averages\n",
    "Nimages = ss[0]- Nave_rolling + 1\n",
    "output_stack_dim = (Nimages,ss[1],ss[2])   # standard size of output rasterstack\n",
    "\n",
    "# define arrays, filled with zeros\n",
    "rasterstack_rolling_HH = np.zeros(output_stack_dim)\n",
    "\n",
    "#if dual pol data. Otherwise, will have to just use HH for classification.\n",
    "\n",
    "rasterstack_rolling_HV = np.zeros(output_stack_dim)\n",
    "rasterstack_rolling_ratio = np.zeros(output_stack_dim)\n",
    "#options:  product of HH and HV, or Sum of HH and HV\n",
    "#here, using product.\n",
    "rasterstack_rolling_prod = np.zeros(output_stack_dim)\n",
    "\n",
    "for ii in range(len(rasterstack_HH)):\n",
    "\n",
    "    for jj in range(-Nave_rolling+1,1):  # example:  -1 to 1\n",
    "        kk = ii + jj\n",
    "        if(kk>=0 and kk<Nimages):\n",
    "            # print(ii,jj,kk)   # ii - input image, kk - output image\n",
    "            # note that the rolling average for image 0 contains images 0, 1, etc., up until the number of averages,\n",
    "            #      but no earlier than image 0\n",
    "            rasterstack_rolling_HH[kk,:,:] = rasterstack_rolling_HH[kk,:,:]+rasterstack_HH[ii,:,:]\n",
    "            rasterstack_rolling_HV[kk,:,:] = rasterstack_rolling_HV[kk,:,:]+rasterstack_HV[ii,:,:]\n",
    "            rasterstack_rolling_ratio[kk,:,:] = np.divide(rasterstack_rolling_HH[kk,:,:],rasterstack_rolling_HV[kk,:,:])+np.divide(rasterstack_HH[ii,:,:],rasterstack_HV[ii,:,:])\n",
    "            rasterstack_rolling_prod[kk,:,:] = np.multiply(rasterstack_rolling_HH[kk,:,:],rasterstack_rolling_HV[kk,:,:])+np.multiply(rasterstack_HH[ii,:,:],rasterstack_HV[ii,:,:])\n",
    "\n",
    "rasterstack_rolling_HH[:,:,:]=rasterstack_rolling_HH[:,:,:]/Nave_rolling\n",
    "rasterstack_rolling_HV[:,:,:]=rasterstack_rolling_HV[:,:,:]/Nave_rolling\n",
    "rasterstack_rolling_ratio[:,:,:]=rasterstack_rolling_ratio[:,:,:]/Nave_rolling\n",
    "rasterstack_rolling_prod[:,:,:]=rasterstack_rolling_prod[:,:,:]/Nave_rolling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac67f82",
   "metadata": {},
   "source": [
    "<a id=\"SEC_2.4\"></a>\n",
    "# 2.4  &emsp; Option: calculate correction factors to insure images are relatively calibrated to one another overall\n",
    "\n",
    "<bold>TODO: optionally select region to calculate normalization factor</bold>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1735fa7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#assumes minor over brightness fluctuations\n",
    "#could be refined to not include veg. inundated areas or areas that are changing significantly\n",
    "#could apply HV correction factor to HH and HV (not affected by inundation, assume similar trends)\n",
    "#the original input arrays are not calibrated\n",
    "\n",
    "#To do:  implement method to apply to image subset only, using a shapefile or pixel coordinates\n",
    "\n",
    "np.seterr(invalid='ignore')\n",
    "\n",
    "# options for normalization:\n",
    "# ical=0 - don't normalize\n",
    "# ical=1 normalize both HH and HV by HV correction factor\n",
    "# ical=2 - normalize each channel separately and individually\n",
    "ical=1\n",
    "\n",
    "#define arrays\n",
    "rasterstack_rolling_corrected_HH = np.zeros(output_stack_dim)\n",
    "rasterstack_rolling_corrected_HV = np.zeros(output_stack_dim)\n",
    "rasterstack_rolling_corrected_ratio = np.zeros(rasterstack_rolling_corrected_HH.shape)\n",
    "rasterstack_rolling_corrected_product = np.zeros(rasterstack_rolling_corrected_HH.shape)\n",
    "\n",
    "#zeros masked out previously\n",
    "mean_HH = np.nanmean(rasterstack_HH)\n",
    "mean_HV = np.nanmean(rasterstack_HV)\n",
    "\n",
    "#print(mean_HH)\n",
    "#print(mean_HV)\n",
    "\n",
    "correction_factor_HH = np.zeros(Nimages)\n",
    "correction_factor_HV = np.zeros(Nimages)\n",
    "print(\"Correction factors for each image:\")\n",
    "print(\" \")\n",
    "\n",
    "print('  #    HHfac    HVfac')\n",
    "print(' ---   -----    -----')\n",
    "for ii in range(len(rasterstack_rolling_HH)):\n",
    "    correction_factor_HH[ii] = np.nanmean(rasterstack_rolling_HH[ii,:,:])/mean_HH\n",
    "    correction_factor_HV[ii] = np.nanmean(rasterstack_rolling_HV[ii,:,:])/mean_HV\n",
    "    if ical == 0:\n",
    "        correction_factor_HH[ii]=1.\n",
    "        correction_factor_HV[ii]=1.\n",
    "    if ical == 1:\n",
    "        correction_factor_HV[ii]=np.nanmean(rasterstack_rolling_HV[ii,:,:])/mean_HV\n",
    "        correction_factor_HH[ii]=correction_factor_HV[ii]\n",
    "    if ical == 2:\n",
    "        correction_factor_HV[ii]=np.nanmean(rasterstack_rolling_HV[ii,:,:])/mean_HV\n",
    "        correction_factor_HH[ii]=np.nanmean(rasterstack_rolling_HH[ii,:,:])/mean_HH\n",
    "\n",
    "    print('%3d   %6.3f   %6.3f' % ( ii, correction_factor_HH[ii], correction_factor_HV[ii]) )\n",
    "    \n",
    "for kk in range(Nimages):\n",
    "    rasterstack_rolling_corrected_HH[kk,:,:] = rasterstack_rolling_HH[kk,:,:]/correction_factor_HH[kk]\n",
    "    rasterstack_rolling_corrected_HV[kk,:,:] = rasterstack_rolling_HV[kk,:,:]/correction_factor_HV[kk]\n",
    "    rasterstack_rolling_corrected_ratio[kk,:,:]=rasterstack_rolling_corrected_HH[kk,:,:]/rasterstack_rolling_corrected_HV[kk,:,:]\n",
    "    rasterstack_rolling_corrected_product[kk,:,:]=rasterstack_rolling_corrected_HH[kk,:,:]*rasterstack_rolling_corrected_HV[kk,:,:]  \n",
    "\n",
    "#remove NaNs\n",
    "rasterstack_rolling_corrected_ratio=np.nan_to_num(rasterstack_rolling_corrected_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb186aa8",
   "metadata": {},
   "source": [
    "# Display corrected rolling averages, channel ratios and products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c884ba7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (40,80)\n",
    "for ii in range(Nimages):\n",
    "\n",
    "    plt.subplot(Nimages,4,ii*4+1)\n",
    "    plt.imshow(rasterstack_rolling_corrected_HH[ii,:,:],vmin=0.0,vmax=0.7,cmap=plt.cm.gray)\n",
    "    plt.title(date_array[ii], loc=\"left\",fontdict = {'fontsize' : 9})\n",
    "    plt.title(date_array[ii+Nave_rolling-1], loc=\"center\",fontdict = {'fontsize' : 9})\n",
    "    plt.title(\"HH avg corr\", loc=\"right\",fontdict = {'fontsize' : 9})\n",
    "    plt.colorbar(shrink=0.4)\n",
    "    plt.subplot(Nimages,4,ii*4+2)\n",
    "    plt.imshow(rasterstack_rolling_corrected_HV[ii,:,:],vmin=0.0,vmax=0.15,cmap=plt.cm.gray)\n",
    "    plt.title(date_array[ii], loc=\"left\",fontdict = {'fontsize' : 9})\n",
    "    plt.title(date_array[ii+Nave_rolling-1], loc=\"center\",fontdict = {'fontsize' : 9})\n",
    "    plt.title(\"HV avg Corr\", loc=\"right\",fontdict = {'fontsize' : 9})\n",
    "    plt.colorbar(shrink=0.4)\n",
    "    plt.subplot(Nimages,4,ii*4+3)\n",
    "    plt.imshow(rasterstack_rolling_corrected_HH[ii,:,:]/rasterstack_rolling_corrected_HV[ii,:,:],vmin=4,vmax=12,cmap=plt.cm.gray)\n",
    "    plt.title(date_array[ii], loc=\"left\",fontdict = {'fontsize' : 9})\n",
    "    plt.title(date_array[ii+Nave_rolling-1], loc=\"center\",fontdict = {'fontsize' : 9})\n",
    "    plt.title(\"Ratio\", loc=\"right\",fontdict = {'fontsize' : 9})\n",
    "    plt.colorbar(shrink=0.4)\n",
    "    plt.subplot(Nimages,4,ii*4+4)\n",
    "    plt.imshow(rasterstack_rolling_corrected_product[ii,:,:],vmin=0,vmax=.0002,cmap=plt.cm.gray)\n",
    "    plt.title(date_array[ii], loc=\"left\",fontdict = {'fontsize' : 9})\n",
    "    plt.title(date_array[Nave_rolling-1], loc=\"center\",fontdict = {'fontsize' : 9})\n",
    "    plt.title(\"Product\", loc=\"right\",fontdict = {'fontsize' : 9})\n",
    "    plt.colorbar(shrink=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cd242c",
   "metadata": {},
   "source": [
    "# Calculate the mean for each stack of images, and the change from the mean for each time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c387101f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Still to be implemented:  change relative to prior image\n",
    "\n",
    "change_HH = np.zeros(output_stack_dim)\n",
    "change_HV = np.zeros(output_stack_dim)\n",
    "change_ratio = np.zeros(output_stack_dim)\n",
    "change_product = np.zeros(output_stack_dim)\n",
    "\n",
    "rasterstack_HH=np.nan_to_num(rasterstack_HH)\n",
    "rasterstack_HV=np.nan_to_num(rasterstack_HV)\n",
    "\n",
    "#check if nans are present\n",
    "array_sum = np. sum(rasterstack_HH)\n",
    "array_has_nan = np. isnan(array_sum)\n",
    "print('Are there any NaNs in HH?', array_has_nan)\n",
    "array_sum = np. sum(rasterstack_HV)\n",
    "array_has_nan = np. isnan(array_sum)\n",
    "print('Are there any NaNs in HV?', array_has_nan)\n",
    "\n",
    "\n",
    "#disable warning: all numbers in stack at one pixel are zero -> causes warning\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    mean_stack_HH = np.mean(rasterstack_HH, axis=0,where=np.greater(rasterstack_HH,0.))\n",
    "    mean_stack_HV = np.mean(rasterstack_HV, axis=0,where=np.greater(rasterstack_HV,0.))\n",
    "    \n",
    "mean_stack_HH=np.nan_to_num(mean_stack_HH)\n",
    "mean_stack_HV=np.nan_to_num(mean_stack_HV)\n",
    "      \n",
    "mean_stack_HH=np.nan_to_num(mean_stack_HH)\n",
    "mean_stack_HV=np.nan_to_num(mean_stack_HV)\n",
    "\n",
    "output_dim = (ss[1],ss[2])  # standard size of output rasterstack\n",
    "\n",
    "# define arrays, filled with zeros\n",
    "mean_prod = np.zeros(output_dim)\n",
    "mean_ratio = np.zeros(output_dim)\n",
    "\n",
    "mean_prod[:,:]=(mean_stack_HH[:,:]*mean_stack_HV[:,:])\n",
    "mean_ratio=mean_stack_HH[:,:]/mean_stack_HV[:,:]\n",
    "\n",
    "for kk in range(Nimages):\n",
    "#relative to mean\n",
    "    change_HH[kk,:,:]=rasterstack_rolling_corrected_HH[kk,:,:]/mean_stack_HH\n",
    "    change_HV[kk,:,:]=rasterstack_rolling_corrected_HV[kk,:,:]/mean_stack_HV\n",
    "    change_ratio[kk,:,:]=rasterstack_rolling_corrected_ratio[kk,:,:]/mean_ratio\n",
    "    change_product[kk,:,:]=rasterstack_rolling_corrected_product[kk,:,:]/mean_prod\n",
    "\n",
    "change_HH=np.nan_to_num(change_HH)\n",
    "change_HV=np.nan_to_num(change_HV)\n",
    "change_ratio=np.nan_to_num(change_ratio)\n",
    "change_product=np.nan_to_num(change_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d1f1d6",
   "metadata": {},
   "source": [
    "# Display change over time of corrected rolling averages, channel ratios and products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d823d3d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Change relative to mean values\")\n",
    "plt.rcParams['figure.figsize'] = (40,50)\n",
    "for ii in range(Nimages):\n",
    "    plt.subplot(Nimages,4,ii*4+1)\n",
    "    plt.imshow(change_HH[ii,:,:],vmin=0.5,vmax=1.5,cmap=plt.cm.gray)\n",
    "    plt.title(date_array[ii], loc=\"left\",fontdict = {'fontsize' : 9})\n",
    "    plt.title(date_array[ii+Nave_rolling-1], loc=\"center\",fontdict = {'fontsize' : 9})\n",
    "    plt.title(\"HH /mean_HH\", loc=\"right\",fontdict = {'fontsize' : 9})\n",
    "    plt.colorbar(shrink=0.4)\n",
    "    plt.subplot(Nimages,4,ii*4+2)\n",
    "    plt.imshow(change_HV[ii,:,:],vmin=0.5,vmax=1.5,cmap=plt.cm.gray)\n",
    "    plt.title(date_array[ii], loc=\"left\",fontdict = {'fontsize' : 9})\n",
    "    plt.title(date_array[ii+Nave_rolling-1], loc=\"center\",fontdict = {'fontsize' : 9})\n",
    "    plt.title(\"HV /mean_HV\", loc=\"right\",fontdict = {'fontsize' : 9})\n",
    "    plt.colorbar(shrink=0.4)\n",
    "    plt.subplot(Nimages,4,ii*4+3)\n",
    "    plt.imshow(change_ratio[ii,:,:],vmin=0.5,vmax=1.5,cmap=plt.cm.gray)\n",
    "    plt.title(date_array[ii], loc=\"left\",fontdict = {'fontsize' : 9})\n",
    "    plt.title(date_array[ii+Nave_rolling-1], loc=\"center\",fontdict = {'fontsize' : 9})\n",
    "    plt.title(\"Ratio / mean ratio\", loc=\"right\",fontdict = {'fontsize' : 9})\n",
    "    plt.colorbar(shrink=0.4)\n",
    "    plt.subplot(Nimages,4,ii*4+4)\n",
    "    plt.imshow(change_product[ii,:,:],vmin=0.5,vmax=1.5,cmap=plt.cm.gray)\n",
    "    plt.title(date_array[ii], loc=\"left\",fontdict = {'fontsize' : 9})\n",
    "    plt.title(date_array[ii+Nave_rolling-1], loc=\"center\",fontdict = {'fontsize' : 9})\n",
    "    plt.title(\"Product / mean product\", loc=\"right\",fontdict = {'fontsize' : 9})\n",
    "    plt.colorbar(shrink=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cddf3be-ab14-4d17-bb39-b404cf060c1c",
   "metadata": {},
   "source": [
    "# Display mean values of HH, HV, products, and ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7701dba-0ff9-4a36-982b-46c1afa5c199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig,[[ax1,ax2],[ax3,ax4]] = plt.subplots(2,2,figsize=(10,10))\n",
    "cax = ax1.imshow(mean_stack_HH,vmin=0,vmax=0.5)\n",
    "ax1.set_title('HH')\n",
    "plt.colorbar(cax,ax=ax1,shrink = 0.3)\n",
    "cax = ax2.imshow(mean_stack_HV,vmin=0,vmax=0.1)\n",
    "ax2.set_title('HV')\n",
    "plt.colorbar(cax,ax=ax2,shrink = 0.3)\n",
    "cax = ax3.imshow(mean_prod,vmin=0,vmax=0.01)\n",
    "ax3.set_title('HH*HV')\n",
    "plt.colorbar(cax,ax=ax3,shrink = 0.3)\n",
    "cax = ax4.imshow(mean_ratio,vmin=2,vmax=10)\n",
    "ax4.set_title('HH/HV')\n",
    "plt.colorbar(cax,ax=ax4,shrink = 0.3)\n",
    "# plt.imshow(mean_prod,vmin=0,vmax=.01)\n",
    "# plt.colorbar(shrink=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9b713f",
   "metadata": {},
   "source": [
    "<a id=\"SEC_3\"></a>\n",
    "# 3 &emsp; Classification\n",
    "\n",
    "If the inundation state is relatively constant over the time span of the imagery, these classes may be a useful reference. However, if the inundation state is variable over the time span, this classification may be biased depending on the classification thresholds and the magnitude of the changes in backscatter.<br>\n",
    "For classifying open water, HV in many cases is a more reliable indicator that HH.  However, to take advantage of both HH and HV, we find the product and threshold based on the product.<br>\n",
    "For classifying inundated vegetation, typically we find that double bounce will dominate the HH backscatter and HH will be brighter than expected from the HV backscatter, which is dominated by volume scatter even in the presence of double bounce reflections occuring.  Since HH also gets brighter and darker as biomass increases or decreases, we have found HH/HV to be a valuable metric for identifying inundated vegatation, even for those cases were the double bounce contribution to HH may be smaller in herbaceous areas (for which we assign a separate class).  We therefore apply thresholds based on HH and HH/HV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff7da7b",
   "metadata": {},
   "source": [
    "# Inundation classes\n",
    "\n",
    "The basis of the classification will be the simplest possible technique to demonstrate sensitivity to inundation state: identifying threshold values in the imagery that correspond different inundation states. A possible augmention would be to first segment the image values by pixel similarity.  \n",
    "\n",
    "(not all values currently used)\n",
    "\n",
    "#0 no data or not classified<br>\n",
    "#1 - 5 open water<br>\n",
    "#7 - 9  no surface water<br>\n",
    "#11 - 13 is inundated veg due to high HH/HV <br>\n",
    "#15 - 17 is inundated veg with bright HH return<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ee29fa-8d1a-4f42-a8f9-f6d1ae50d5fd",
   "metadata": {},
   "source": [
    "<a id=\"SEC_3.1\"></a>\n",
    "# 3.1  &emsp; Set thresholds and classify mean image\n",
    "\n",
    "<bold>TODO: optionally select region to calculate normalization factor</bold>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf638f83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.seterr(invalid='ignore')  # python doesn't like dividing zero by zero, NaN by NaN, etc.\n",
    "\n",
    "#Thresholds to be calibrated based on other data sets:\n",
    "# open water class 1\n",
    "T1_prod_min =  0.000001#0.000001\n",
    "T1_prod_max =  0.0001#.00005\n",
    "# inundated veg class 16 - bright HH\n",
    "T2_HH_min =  0.1 #0.5\n",
    "T2_HH_max =  0.5#10.\n",
    "T2_ratio_min= 6 #6\n",
    "T2_ratio_max=100 #500\n",
    "# inundated veg class 12  - not as bright HH in class 16, but high HH/HV ratio\n",
    "T3_HH_min =  0.001#0.1\n",
    "T3_HH_max =  0.2#0.3\n",
    "T3_ratio_min= 4 #8\n",
    "T3_ratio_max=500\n",
    "\n",
    "classif=np.zeros(output_dim, dtype=np.int8)\n",
    "\n",
    "#all data\n",
    "classif=np.where(mean_stack_HH > 0.0,8,classif)\n",
    "#open water\n",
    "classif=np.where((mean_prod < T1_prod_max) & (mean_prod > T1_prod_min),3,classif)\n",
    "#classify inundated veg where not classified as open water\n",
    "classif=np.where((classif != 3)  & (mean_stack_HH > T2_HH_min) & (mean_stack_HH < T2_HH_max) \\\n",
    "                 &(mean_ratio < T2_ratio_max) & (mean_ratio > T2_ratio_min),16,classif)\n",
    "\n",
    "# classif=np.where((classif != 3)  & (mean_stack_HH > T3_HH_min) & (mean_stack_HH < T3_HH_max) \\\n",
    "#                  &(mean_ratio < T3_ratio_max) & (mean_ratio > T3_ratio_min),12,classif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8355a3",
   "metadata": {},
   "source": [
    "# Display classification of mean image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020c3950",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12,40)\n",
    "\n",
    "#define color table for this and subsequent plots\n",
    "colors =[\"None\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",'green','green','green','green','yellow','yellow','yellow','yellow',\n",
    "         'gold','gold','gold','gold']\n",
    "cmap = ListedColormap(colors)\n",
    "\n",
    "# plt.subplot(Nimages,1,ii+1)\n",
    "cax = plt.imshow(classif,vmin=1,vmax=16,cmap=cmap,interpolation='none')\n",
    "plt.title(date_array[0], loc=\"left\",fontdict = {'fontsize' : 9})\n",
    "plt.title(date_array[Nimages-1], loc=\"center\",fontdict = {'fontsize' : 9})\n",
    "plt.title(\"class\", loc=\"right\",fontdict = {'fontsize' : 9})\n",
    "# plt.colorbar(shrink=0.1)\n",
    "\n",
    "cbar = fig.colorbar(cax, ticks=[4,8,11,14],shrink=0.1)\n",
    "cbar.ax.set_yticklabels(['Open Water','No Surface Water','Inundated Vegetation (High HH:HV)','Inundated Vegetation (High HH)'])  # vertically oriented colorbar\n",
    "                         \n",
    "                         \n",
    "                         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3626c930",
   "metadata": {},
   "source": [
    "<a id=\"SEC_3.2\"></a>\n",
    "# 3.2  &emsp; Classify Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e1e258",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#classify time series\n",
    "\n",
    "#Thresholds to be calibrated based on other data sets:\n",
    "# open water class 1\n",
    "T1_prod_min =  0.000001\n",
    "T1_prod_max =  .00005\n",
    "# inundated veg class 16 - bright HH\n",
    "T2_HH_min =  0.5\n",
    "T2_HH_max =  10.\n",
    "T2_ratio_min=6\n",
    "T2_ratio_max=500\n",
    "# inundated veg class 12  - not as bright HH in class 16, but has high HH/HV ratio\n",
    "T3_HH_min =  0.1\n",
    "T3_HH_max =  0.3\n",
    "T3_ratio_min=8\n",
    "T3_ratio_max=500\n",
    "\n",
    "class_stack=np.zeros(output_stack_dim,dtype=np.int8)\n",
    "change_OW=np.zeros(class_stack.shape[0],dtype=np.int32)\n",
    "change_FL=np.zeros(class_stack.shape[0],dtype=np.int32)\n",
    "change_NF=np.zeros(class_stack.shape[0],dtype=np.int32)\n",
    "\n",
    "for kk in range(len(class_stack)):\n",
    "    class_stack[kk,:,:]=np.where(rasterstack_rolling_corrected_HH[kk,:,:] > 0.0000001,8,class_stack[kk,:,:])\n",
    "\n",
    "    class_stack[kk,:,:]=np.where((rasterstack_rolling_corrected_product[kk,:,:] < T1_prod_max) \\\n",
    "                                 & (rasterstack_rolling_corrected_product[kk,:,:] > T1_prod_min),3,class_stack[kk,:,:])\n",
    "\n",
    "#classify inundated veg where not classified as open water\n",
    "    class_stack[kk,:,:]=np.where((class_stack[kk,:,:] != 3)  \\\n",
    "                                 & (rasterstack_rolling_corrected_HH[kk,:,:] > T2_HH_min) \\\n",
    "                                 & (rasterstack_rolling_corrected_HH[kk,:,:] < T2_HH_max) \\\n",
    "                                 &(rasterstack_rolling_corrected_ratio[kk,:,:] < T2_ratio_max) \\\n",
    "                                 & (rasterstack_rolling_corrected_ratio[kk,:,:] > T2_ratio_min),16,class_stack[kk,:,:])\n",
    "\n",
    "\n",
    "    class_stack[kk,:,:]=np.where((class_stack[kk,:,:] != 3)  \\\n",
    "                                  & (rasterstack_rolling_corrected_HH[kk,:,:] > T3_HH_min) \\\n",
    "                                  & (rasterstack_rolling_corrected_HH[kk,:,:] < T3_HH_max) \\\n",
    "                                  &(rasterstack_rolling_corrected_ratio[kk,:,:] < T3_ratio_max) \\\n",
    "                                  & (rasterstack_rolling_corrected_ratio[kk,:,:] > T3_ratio_min),12,class_stack[kk,:,:])\n",
    "\n",
    "    #track changes\n",
    "    change_OW[kk]=np.count_nonzero(class_stack[kk,:,:] == 3)\n",
    "    \n",
    "    change_FL[kk]=np.count_nonzero(class_stack[kk,:,:] == 12)+np.count_nonzero(class_stack[kk,:,:] == 16)\n",
    "    \n",
    "    change_NF[kk]=np.count_nonzero(class_stack[kk,:,:] == 8)\n",
    "\n",
    "#    print(f\"Number of Zeroes in Array --> {class_stack[kk,:,:][np.where( class_stack[kk,:,:]== 0)].size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02dc1ae",
   "metadata": {},
   "source": [
    "# Display change in inundation vs time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a045bbf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sanity check\n",
    "plt.rcParams['figure.figsize'] = (6,4)\n",
    "fig = plt.figure()\n",
    "X = np.arange(class_stack.shape[0])\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "classn=[0,1,2,3,4]\n",
    "colors = {'Not Inundated':'green', 'Open Water':'blue','Inundated Veg':'yellow'}         \n",
    "labels = list(colors.keys())\n",
    "handles = [plt.Rectangle((0,0),1,1, color=colors[label]) for label in labels]\n",
    "plt.legend(handles, labels)\n",
    "#ax.legend(labels=['Not Flooded','Open Water', 'Inundated Veg'])\n",
    "ax.bar(X+0.50,change_NF,color='green',width=0.25)\n",
    "ax.bar(X+0.00,change_OW,color='blue',width=0.25)\n",
    "ax.bar(X+0.25,change_FL,color='yellow',width=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b68ac",
   "metadata": {},
   "source": [
    "# Display time sequence of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4eae98-38b5-4709-b540-b37d8b2f9d2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "date_array[ii+Nave_rolling-1].strftime(\"%m/%d/%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc8804f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (8,32)\n",
    "import imageio\n",
    "with imageio.get_writer('%s/inundation_classes.gif' %(aoi_dir),mode='I',duration=300) as writer:\n",
    "    for ii in range(Nimages):\n",
    "\n",
    "        plt.subplot(Nimages,1,ii+1)\n",
    "\n",
    "        plt.imshow(class_stack[ii,:,:],vmin=1,vmax=16,cmap=cmap,interpolation='none')\n",
    "        plt.title(date_array[ii].strftime(\"%m/%d/%Y\"), loc=\"left\",fontdict = {'fontsize' : 9})\n",
    "        plt.title(date_array[ii+Nave_rolling-1].strftime(\"%m/%d/%Y\"), loc=\"center\",fontdict = {'fontsize' : 9})\n",
    "        plt.title(\"class\", loc=\"right\",fontdict = {'fontsize' : 9})\n",
    "        plt.colorbar(shrink=0.8)\n",
    "\n",
    "        plt.savefig('%s/%s.png' %(aoi_dir,date_array[ii].strftime(\"%Y%m%d\")))\n",
    "        image = imageio.v2.imread('%s/%s.png' %(aoi_dir,date_array[ii].strftime(\"%Y%m%d\")))\n",
    "        writer.append_data(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffce90b",
   "metadata": {},
   "source": [
    "<a id=\"SEC_3.3\"></a>\n",
    "# 3.3  &emsp; Examine change relative to means for each time stamp to refine classes\n",
    "\n",
    "Change in HH backscatter and in HH/HV from the mean or the previous HH and HH/HV values can be a more robust indicator of a change occuring in class. Thresholds are applied based on those change metrics.\n",
    "\n",
    "Not yet implemented option: change focused on changes from typical inundation state derived from the mean imagery over the entire time span. Detect change in backscatter for each time stamp within identified classes from mean imagery to refine classes. This will be an augmentation to the change detection implemented below for all areas and is TBD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c82bb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "change_OW_2=np.zeros(class_stack.shape[0],dtype=np.int32)\n",
    "change_FL_2=np.zeros(class_stack.shape[0],dtype=np.int32)\n",
    "change_NF_2=np.zeros(class_stack.shape[0],dtype=np.int32)\n",
    "\n",
    "T5_HH_inc=2\n",
    "T5_HH_dec=0.5\n",
    "T5_ratio_inc=2\n",
    "T5_ratio_dec=0.5\n",
    "\n",
    "T6_HH_inc=2\n",
    "T6_HH_dec=0.5\n",
    "T6_ratio_inc=2\n",
    "T6_ratio_dec=0.5\n",
    "\n",
    "T7_HH_inc=4\n",
    "T7_HH_dec=0.25\n",
    "T7_ratio_inc=4\n",
    "T7_ratio_dec=0.25\n",
    "\n",
    "for kk in range(Nimages):\n",
    "\n",
    "#open water increased by x, -> no surface water\n",
    "    class_stack[kk,:,:]=np.where((class_stack[kk,:,:] == 3)  \\\n",
    "                                 & (change_HH[kk,:,:] > T5_HH_inc) \\\n",
    "                                 & (change_ratio[kk,:,:] < T5_ratio_inc),7,class_stack[kk,:,:])\n",
    "    class_stack[kk,:,:]=np.where((class_stack[kk,:,:] == 3)  \\\n",
    "                                 & (change_HH[kk,:,:] > T5_HH_inc) \\\n",
    "                                 & (rasterstack_rolling_corrected_ratio[kk,:,:] > T3_ratio_max),11,class_stack[kk,:,:])\n",
    "    \n",
    "#inundated veg decreased by x, -> no surface water\n",
    "    class_stack[kk,:,:]=np.where((class_stack[kk,:,:] == 16)  \\\n",
    "                                 & (change_HH[kk,:,:] < T6_HH_dec) \\\n",
    "                                 & (change_ratio[kk,:,:] < T6_ratio_dec),9,class_stack[kk,:,:])\n",
    "    class_stack[kk,:,:]=np.where((class_stack[kk,:,:] == 12)  \\\n",
    "                                 & (change_HH[kk,:,:] < T6_HH_dec) \\\n",
    "                                 & (change_ratio[kk,:,:] < T6_ratio_dec),9,class_stack[kk,:,:])\n",
    "     \n",
    "#no surface water decreased by x, -> open water\n",
    "    class_stack[kk,:,:]=np.where((class_stack[kk,:,:] == 8)  \\\n",
    "                                 & (change_HH[kk,:,:] < T5_HH_dec) \\\n",
    "                                 & (change_ratio[kk,:,:] > T5_ratio_inc),4,class_stack[kk,:,:])\n",
    "#no surface water increased by x, -> inundated veg\n",
    "    class_stack[kk,:,:]=np.where((class_stack[kk,:,:] == 8)  \\\n",
    "                                 & (change_HH[kk,:,:] > T6_HH_inc) \\\n",
    "                                 & (change_ratio[kk,:,:] > T6_ratio_inc),13,class_stack[kk,:,:])\n",
    "\n",
    "#track changes\n",
    "    change_OW_2[kk]=np.count_nonzero(class_stack[kk,:,:] == 1)+np.count_nonzero(class_stack[kk,:,:] == 2)\\\n",
    "    +np.count_nonzero(class_stack[kk,:,:] == 3)+np.count_nonzero(class_stack[kk,:,:] == 4)\\\n",
    "    +np.count_nonzero(class_stack[kk,:,:] == 5)\n",
    "\n",
    "    change_FL_2[kk]=np.count_nonzero(class_stack[kk,:,:] == 11)+np.count_nonzero(class_stack[kk,:,:] == 12)\\\n",
    "    +np.count_nonzero(class_stack[kk,:,:] == 13)\\\n",
    "    +np.count_nonzero(class_stack[kk,:,:] == 15)\\\n",
    "    +np.count_nonzero(class_stack[kk,:,:] == 16)\\\n",
    "    +np.count_nonzero(class_stack[kk,:,:] == 17)\\\n",
    "    \n",
    "    change_NF_2[kk]=np.count_nonzero(class_stack[kk,:,:] == 7)+np.count_nonzero(class_stack[kk,:,:] == 8)\\\n",
    "    +np.count_nonzero(class_stack[kk,:,:] == 9)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12823f8c",
   "metadata": {},
   "source": [
    "# To assess impact of change detection to classification results, plot the change in number of pixels in each class for each time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97d7323",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#display change in inundation vs time\n",
    "\n",
    "print(\"Change in surface water classes\")\n",
    "plt.rcParams['figure.figsize'] = (6,4)\n",
    "fig = plt.figure()\n",
    "X = np.arange(class_stack.shape[0])\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "classn=[0,1,2,3,4]\n",
    "colors = {'Not Inundated':'green', 'Open Water':'blue','Inundated Veg':'yellow'}         \n",
    "labels = list(colors.keys())\n",
    "handles = [plt.Rectangle((0,0),1,1, color=colors[label]) for label in labels]\n",
    "plt.legend(handles, labels)\n",
    "ax.bar(X+0.50,100*(change_NF_2-change_NF),color='green',width=0.25)\n",
    "ax.bar(X+0.00,100*(change_OW_2-change_OW),color='blue',width=0.25)\n",
    "ax.bar(X+0.25,100*(change_FL_2-change_FL),color='yellow',width=0.25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddd2cdb",
   "metadata": {},
   "source": [
    "# Display final time sequence of wetland inundation classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6429c8c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (8,32)\n",
    "with imageio.get_writer('%s/inundation_classes.gif' %(aoi_dir),mode='I',duration=300) as writer:\n",
    "\n",
    "    for ii in range(Nimages):\n",
    "\n",
    "        # plt.subplot(Nimages,1,ii+1)\n",
    "        fig,ax = plt.subplots(1,1,figsize=(10,10))\n",
    "        im = ax.imshow(class_stack[ii,:,:],vmin=1,vmax=16,cmap=cmap,interpolation='none')\n",
    "        ax.set_title(date_array[ii], loc=\"left\",fontdict = {'fontsize' : 9})\n",
    "        ax.set_title(date_array[ii+Nave_rolling-1], loc=\"center\",fontdict = {'fontsize' : 9})\n",
    "        ax.set_title(\"class\", loc=\"right\",fontdict = {'fontsize' : 9})\n",
    "        # plt.colorbar(im,ax=ax,shrink=0.8)\n",
    "        cbar = fig.colorbar(im, ticks=[4,8,11,14],shrink=0.8)\n",
    "        cbar.ax.set_yticklabels(['Open Water','No Surface Water','Inundated Vegetation (High HH:HV)','Inundated Vegetation (High HH)'])  # vertically oriented colorbar\n",
    "\n",
    "        plt.savefig('%s/%s.png' %(aoi_dir,date_array[ii].strftime(\"%Y%m%d\")))\n",
    "        image = imageio.v2.imread('%s/%s.png' %(aoi_dir,date_array[ii].strftime(\"%Y%m%d\")))\n",
    "        writer.append_data(image)\n",
    "        # plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82faf34",
   "metadata": {},
   "source": [
    "<a id=\"SEC_4\"></a>\n",
    "# 4  &emsp; Output results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfd32d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#output results\n",
    "for kk in range(len(class_stack)):\n",
    "    outname = aoi_dir/ (date_array[kk].strftime(\"%m_%d_%Y\") + '_class.bin')\n",
    "    fid = open(outname,'wb')\n",
    "    print(kk,outname)\n",
    "    tmp = np.byte(np.copy(class_stack[kk,:,:]))\n",
    "    fid.write(tmp)\n",
    "    fid.close();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6f050d",
   "metadata": {},
   "source": [
    "# Plot the area of each class within the imagery for each time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4f17a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#display change in inundation vs time\n",
    "\n",
    "print(\"Change in area of surface water classes over time\")\n",
    "plt.rcParams['figure.figsize'] = (6,4)\n",
    "fig = plt.figure()\n",
    "X = np.arange(class_stack.shape[0])\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "classn=[0,1,2,3,4]\n",
    "colors = {'Not Inundated':'green', 'Open Water':'blue','Inundated Veg':'yellow'}         \n",
    "labels = list(colors.keys())\n",
    "handles = [plt.Rectangle((0,0),1,1, color=colors[label]) for label in labels]\n",
    "plt.legend(handles, labels)\n",
    "ax.bar(X+0.50,change_NF_2,color='green',width=0.25)\n",
    "ax.bar(X+0.00,change_OW_2,color='blue',width=0.25)\n",
    "ax.bar(X+0.25,change_FL_2,color='yellow',width=0.25)\n",
    "plt.savefig(aoi_dir/ 'AreaChange.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb0d69e",
   "metadata": {},
   "source": [
    "# Water level gage data at nearby Vicksburg Bridge on the Mississippi River\n",
    "(water level does not generally correspond linearly to surface water extent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741446e9",
   "metadata": {},
   "source": [
    "https://rivergages.mvr.usace.army.mil/WaterControl/stationinfo2.cfm?sid=CE40FF58&fid=VCKM6&dt=S\n",
    "\n",
    "<img style=\"float: left;\" src=\"images/water_level_chart1.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3b667b-f3c0-43da-a8f2-e9c115e5137a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.local-NISAR_EA_Workshop_2024_10]",
   "language": "python",
   "name": "conda-env-.local-NISAR_EA_Workshop_2024_10-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
